{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7SpEPO_eCUX",
        "outputId": "4a2674c5-dbf5-419b-eb8b-19ac618b590c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2020-01-07 16:08:17,077 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.16/tika-server-1.16.jar to /tmp/tika-server.jar.\n"
          ]
        }
      ],
      "source": [
        "#phone_no extraction.py\n",
        "#test case\n",
        "#RUNNING FINE\n",
        "#import utils\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "#from req_intl.modules.extraction import helpers\n",
        "from urlextract import URLExtract\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from tika import parser\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import tika\n",
        "import os\n",
        "from tika import unpack\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#ph_num=phone_no_extracted[0]['inference']\n",
        "def clean_bookmark_footer_tag(txt):\n",
        "    if type(txt) == list:\n",
        "        # Remove bookmark\n",
        "        txt_trans = [re.sub('bookmark: ', 'bookmark:', item) for item in txt]\n",
        "        txt_trans = [re.sub('(bookmark:)+[_]', '[bookmark:]', item) for item in txt_trans]\n",
        "        txt_trans = [re.sub('(bookmark:\\])+\\w+[\\]]', '[bookmark:]', item) for item in txt_trans]\n",
        "        txt_trans = [re.sub(\"[\\[]{1,}\", '[' , item) for item in txt_trans]\n",
        "        txt_trans = [re.sub(\"(\\[bookmark:)+\\w+(])|(\\[bookmark:])|(bookmark:)+\\w+\\s\", '' , item) for item in txt_trans]\n",
        "        # Strips multi whte spaces into one\n",
        "        txt_trans = [re.sub(' +', ' ', item).strip() for item in txt_trans]\n",
        "        return txt_trans\n",
        "    else:\n",
        "        # Many sections have bookmark in them\n",
        "        txt_trans = re.sub('bookmark: ', 'bookmark:', txt)\n",
        "        txt_trans = re.sub('(bookmark:)+[_]', '[bookmark:]', txt_trans)\n",
        "        txt_trans = re.sub('(bookmark:\\])+\\w+[\\]]', '[bookmark:]', txt_trans)\n",
        "        txt_trans = re.sub(\"[\\[]{1,}\", '[' , txt_trans)\n",
        "        txt_trans = re.sub(\"(\\[bookmark:)+\\w+(])|(\\[bookmark:])|(bookmark:)+\\w+\\s\", '' , txt_trans)\n",
        "        txt_trans = re.sub(' +', ' ', txt_trans).strip()\n",
        "        return txt_trans\n",
        "\n",
        "\n",
        "def clean_multi_spaces_alphanum(txt):\n",
        "    if type(txt) == list:\n",
        "        # This cleans any leading and trailing junk. Excludes the characters given in []\n",
        "        txt_trans = [re.sub(r'^[^a-zA-Z0-9“”•]{1,}|[^a-zA-Z0-9.)(“”]{1,}$',r'', item) for item in txt]\n",
        "        # This removes ending with ()\n",
        "        txt_trans = [re.sub(r'\\(\\)|\\( +\\)',r'', item) for item in txt_trans]\n",
        "        # .net developer is causing issues\n",
        "        txt_trans = [('.' + item) if item.lower().find('net ') == 0 else item for item in txt_trans]\n",
        "        # Strips multi whte spaces into one\n",
        "        txt_trans = [re.sub(' +', ' ', item).strip() for item in txt_trans]\n",
        "\n",
        "        return txt_trans\n",
        "    else:\n",
        "        txt_trans = re.sub(r'^[^a-zA-Z0-9“”•]{1,}|[^a-zA-Z0-9.)(“”]{1,}$',r'', txt)\n",
        "        txt_trans = re.sub(r'\\(\\)|\\( +\\)',r'', txt_trans)\n",
        "        # .net developer is causing issues\n",
        "        if txt_trans.lower().find('net ') == 0:\n",
        "            txt_trans = '.' + txt_trans\n",
        "        # Strips multi whte spaces into one\n",
        "        txt_trans = re.sub(' +', ' ', txt_trans).strip()\n",
        "\n",
        "        return txt_trans\n",
        "\n",
        "\n",
        "def compare_certificate(resume_name,certificate):\n",
        "    for i in df_csv.itertuples():\n",
        "        if i._1 == resume_name:\n",
        "            pd_certificate =str(i.Certification)\n",
        "            match_certificate = fuzz.ratio(pd_certificate , certificate)\n",
        "        #    print(\"\\n COMPARE CERTIFICATE:\")\n",
        "        #    print(\"extracted :\\t\"+certificate)\n",
        "        #    print(\"csv_certificate:\"+pd_certificate)\n",
        "         #   print(\"fuzzy_match:\\t\"+str(match_certificate))\n",
        "          # print(str(resume_name)+\"\\t\"+pd_certificate+\"\\t\"+str(certificate))\n",
        "            if (pd_certificate==certificate):\n",
        "                return 1\n",
        "            elif match_certificate>80:\n",
        "                return 1\n",
        "            elif (certificate == '' and pd_certificate=='nan'):\n",
        "                return 1\n",
        "            elif (certificate == '' and pd_certificate=='None'):\n",
        "                return 1\n",
        "            elif(len(certificate)==0 and len(pd_certificate)!=0): #extracted phone no is empty but value exists in csv\n",
        "                return 0\n",
        "            else :\n",
        "                return 0\n",
        "\n",
        "\n",
        "\n",
        "def compare_phone(resume_name,ph_num_extract):\n",
        "    ph=ph_num_extract\n",
        "    ph_num= \"\".join(c for c in ph if c not in ('!','.',':',\"-\",\" \"))\n",
        "  #  print(ph_num)\n",
        "       # print(j)\n",
        "\n",
        "    for i in df_csv.itertuples():\n",
        "\n",
        "\n",
        "        if i._1== resume_name:\n",
        "          #  print('name')\n",
        "            pd_phone=str(i.Phone)\n",
        "            pd_phone_no = \"\".join(c for c in pd_phone if c not in ('!','.',':',\"-\",\" \"))\n",
        "\n",
        "            #  print(\"yes\")\n",
        "          # print(pd_phone_no)\n",
        "          # print(ph_num)\n",
        "            match_phone_no=fuzz.ratio(pd_phone_no,ph_num)\n",
        "        #    print(\"phone no :\\t\"+\"\\t\"+pd_phone_no+\"\\t\"+str(ph_num))\n",
        "         #   print(match)\n",
        "            if(pd_phone_no==ph_num): # direct match\n",
        "                return 1\n",
        "            elif match_phone_no>85:   #similarity match\n",
        "                return 1\n",
        "            # print(\"Extracted from resume:\\t\"+ph_num)\n",
        "             #  print(\"percentage of matching\\t:\"+match)\n",
        "            elif (ph_num=='' and pd_phone_no=='nan'): #both are empty values\n",
        "                return 1\n",
        "            elif(len(ph_num)==0 and len(pd_phone_no)!=0): #extracted phone no is empty but value exists in csv\n",
        "                return 0\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "\n",
        "\n",
        "def compare_email(resume_name,email_extract):\n",
        "    email=email_extract.lower()\n",
        "    for i in df_csv.itertuples():\n",
        "        if i._1==resume_name:\n",
        "            pd_email=str(i.Email)\n",
        "            csv_email=pd_email.lower()\n",
        "            match_email=fuzz.ratio(email,csv_email)\n",
        "          #  print(\"email:\\t\"+\"\\t\"+csv_email+\"\\t\"+str(email))\n",
        "\n",
        "            if(csv_email== email):\n",
        "                return 1\n",
        "            elif match_email > 85:\n",
        "                return 1\n",
        "            elif (email=='' and csv_email=='nan' ): #both are empty values\n",
        "                return 1\n",
        "            elif (email=='' and csv_email=='none'):\n",
        "                return 1\n",
        "            elif(len(email)==0 and len(csv_email)!=0): #extracted phone no is empty but value exists in csv\n",
        "                return 0\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "def compare_linked_in(resume_name,linked_in):\n",
        "\n",
        "    for i in df_csv.itertuples():\n",
        "\n",
        "\n",
        "        if i._1==resume_name:\n",
        "            if linked_in != []:\n",
        "                linked_in_extract = linked_in[0]['inference']\n",
        "                pd_linked_in=str(i.LinkedIn)\n",
        "                csv_linked_in=pd_linked_in.lower()\n",
        "                match_linked_in=fuzz.ratio(csv_linked_in,linked_in_extract)\n",
        "                if(csv_linked_in == linked_in_extract):\n",
        "                    return 1\n",
        "                elif match_linked_in > 85:\n",
        "                    return 1\n",
        "                else:\n",
        "                    return 0\n",
        "              #  print(\"linked_in:\\t\"+\"\\t\"+csv_linked_in+\"\\t\"+str(linked_in_extract))\n",
        "        if i._1==resume_name:\n",
        "            if linked_in == []:\n",
        "\n",
        "                pd_linked_in=str(i.LinkedIn)\n",
        "                csv_linked_in=pd_linked_in.lower()\n",
        "\n",
        "                if (linked_in ==[] and csv_linked_in =='nan'):\n",
        "                    return 1\n",
        "                elif(linked_in == [] and len(csv_linked_in)!=0):\n",
        "                    return 0\n",
        "                else:\n",
        "                    return 0\n",
        "              #  print(\"linked_in:\\t\"+\"\\t\"+csv_linked_in+\"\\t\"+str(linked_in))\n",
        "\n",
        "def resume_output_cleaner(text_content, field_name):\n",
        "    if text_content:\n",
        "\n",
        "        if field_name == \"candidate_name\":\n",
        "            #text_content = ' '.join(pattern.findall(text_content))\n",
        "            text_content = clean_multi_spaces_alphanum(text_content)\n",
        "            text_content = text_content[:100]\n",
        "            return text_content.title()\n",
        "\n",
        "        elif field_name == \"candidate_email\":\n",
        "            # text_content = str(re.match('\\w ')).strip()\n",
        "            text_content = text_content[:50]\n",
        "            return text_content\n",
        "\n",
        "        elif field_name == \"candidate_phone_number\":\n",
        "            text_content = text_content[:50]\n",
        "            return text_content\n",
        "\n",
        "        # Error: Do not remove unicode in address\n",
        "        elif field_name == \"candidate_address\":\n",
        "            # text_content = ' '.join(pattern.findall(text_content))\n",
        "            text_content = text_content[:200]\n",
        "            return text_content\n",
        "\n",
        "\n",
        "        elif field_name == \"linkedin\":\n",
        "            text_content = text_content[:100]\n",
        "            return text_content\n",
        "\n",
        "        elif field_name == \"summary\":\n",
        "            return clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "\n",
        "\n",
        "        elif field_name == \"critical_skill_matrix\":\n",
        "            return clean_bookmark_footer_tag(text_content)\n",
        "\n",
        "\n",
        "        elif field_name == \"certificate\":\n",
        "            return clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "\n",
        "\n",
        "        elif field_name == \"skill_matrix\":\n",
        "            return clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "\n",
        "\n",
        "        elif field_name == \"education\":\n",
        "            return clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "\n",
        "\n",
        "        elif field_name == \"client_name\":\n",
        "            #text_content = ' '.join(pattern.findall(text_content))\n",
        "            text_content = clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "            text_content = text_content[:100]\n",
        "            return text_content\n",
        "\n",
        "\n",
        "        elif field_name == \"location\":\n",
        "            #text_content = ' '.join(pattern.findall(text_content))\n",
        "            if ',' in text_content:\n",
        "                text_content = re.sub(r',', r', ', text_content)\n",
        "            text_content = clean_multi_spaces_alphanum(clean_bookmark_footer_tag(text_content))\n",
        "            text_content = text_content[:70]\n",
        "            return text_content\n",
        "\n",
        "\n",
        "        elif field_name == \"role\":\n",
        "            text_content = clean_bookmark_footer_tag(clean_multi_spaces_alphanum(text_content))\n",
        "            text_content = text_content[:100]\n",
        "\n",
        "            if any(ch in text_content for ch in ['/',',','-']):\n",
        "                return text_content\n",
        "            else:\n",
        "                return case_formatting(text_content)\n",
        "\n",
        "\n",
        "        elif field_name == \"duration\":\n",
        "            text_content = clean_bookmark_footer_tag(clean_multi_spaces_alphanum(text_content))\n",
        "\n",
        "            #print(\"Actual Date: \" + text_content)\n",
        "            text_content = date_standardization(text_content)\n",
        "            #print(\"Standardized Date: \" + text_content)\n",
        "\n",
        "            text_content = text_content[:50]\n",
        "            return text_content\n",
        "\n",
        "\n",
        "        elif field_name == \"responsibilities\":\n",
        "            text_content = clean_bookmark_footer_tag(clean_multi_spaces_alphanum(text_content))\n",
        "            return text_content\n",
        "\n",
        "\n",
        "        elif field_name == \"environment\":\n",
        "            text_content = clean_bookmark_footer_tag(clean_multi_spaces_alphanum(text_content))\n",
        "            return text_content\n",
        "    else:\n",
        "        return text_content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_cleaned_encoded_text_from_jd(text_content):\n",
        "\n",
        "    ''' input type - str\n",
        "        output type - list '''\n",
        "\n",
        "    text = []\n",
        "\n",
        "    text_content = re.sub(r'\\[\\w*:?\\s*.*\\]','',str(text_content))   #[helps in eliminating [boundary_ image] images ,bookmarks and images\n",
        "\n",
        "    for line in text_content.split('\\n'):\n",
        "        line = unicodedata.normalize(\"NFKD\",line)  #replaces \\xa0 with space\n",
        "        line = line.replace('–','-').encode('ascii','ignore').decode('utf-8').strip()    # encoding the line ,to remove unicode characters\n",
        "        if line != '':  #after encoding checking if its empty line\n",
        "            line = re.sub(r'[\\t\\r\\f\\v\\s]{1,}',' ',line)        #removing all types of spaces\n",
        "            text.append(line)\n",
        "\n",
        "    return text\n",
        "def get_first_block_from_resume(text_content):\n",
        "\n",
        "\n",
        "    text_content = re.sub(r'\\[\\w*:?\\s*.*\\]','',str(text_content)) #boundary images\n",
        "    exclude_list = ['key accomplishments', 'employment','professional summary' 'scholastics', 'online portfolio', 'declaration', 'academia', 'education/ certification', 'work experience', 'references', 'experience', 'tools and technologies', 'skills set', 'skills', 'workshops', 'key technologies', 'volunteer', 'technical expertise', 'languages and technologies', 'qualification', 'education/', 'competencies', 'skills:', 'academic credentials', 'technical skills', 'additional information', 'education:-', 'certification:', 'academic profile', 'education and certification', 'summary', 'certificates', 'certifications/licenses', 'skill set', 'education & certification', 'certifications', 'technical skill sets', 'software', 'seminars', 'educational', 'languages and tools', 'graduation', 'automation tools', 'objective', 'education / certification', 'academic background', 'achievements', 'technology and tools', 'skill matrix', 'tools and technology', 'technical conferences', 'projects overview', 'online profile links', 'responsibilities', 'technical set', 'academic projects', 'work', 'skills acquired', 'training', 'certification', 'certificate', 'presentations', 'technical', 'skills summary', 'tools', 'education:', 'portfolio', 'status', 'trainings', 'technical summary', 'technical proficiencies', 'academic', 'qualifications', 'publications', 'technical proficiency', 'awards', 'academic qualification', 'hobbies', 'groups', 'programming', 'technical skill set', 'rewards and achievements', 'core competencies', 'areas of specialization', 'academic qualifications', 'education /certification', 'projects', 'education/certification', 'technologies', 'detailed tech stack', 'education', 'technical sets', 'corporate', 'professional experience', 'languages', 'personal', 'technology summary', 'areas of experitse', 'military', 'certifications:']\n",
        "\n",
        "    mwetokenizer = MWETokenizer(separator=' ')\n",
        "    for each_word in exclude_list:\n",
        "        mwetokenizer.add_mwe(each_word.split(\" \"))\n",
        "\n",
        "    first_block = []\n",
        "\n",
        "    for index,each_line in enumerate(text_content.split('\\n')[:15]):\n",
        "\n",
        "        each_line = each_line.lower()\n",
        "        tokens = wordpunct_tokenize(each_line)\n",
        "        tokens = mwetokenizer.tokenize(tokens)\n",
        "        if tokens != [] :\n",
        "            if tokens[0].strip() in exclude_list:\n",
        "                first_block = text_content.split('\\n')[:index]\n",
        "                break\n",
        "            else:\n",
        "                first_block =  text_content.split('\\n')[:15]\n",
        "\n",
        " #   print(first_block)\n",
        "    return first_block\n",
        "email_pattern = re.compile(r'([\\w\\-\\.]+@(\\w[\\w\\-]+\\.)+[\\w\\-]+)',re.IGNORECASE)\n",
        "linkedin_pattern = re.compile(r'((ftp|http|https):\\/\\/?)?((www|\\w\\w)\\.)?linkedin.com(\\w+:{0,1}\\w*@)?(\\S+)(:([0-9])+)?(\\/|\\/([\\w#!:.?+=&%@!\\-\\/]))?')\n",
        "\n",
        "def feeder_certificate(file_path, text_content):\n",
        "#    # Uncomment for standalone testing\n",
        "#    text_content = conv_tika_to_text(file_path)\n",
        "\n",
        "    # Determine delimiter for sections\n",
        "    text_temp_replaced = re.sub(r'\\n \\n', r'\\n\\n', text_content)\n",
        "    text_temp = re.sub(r'\\n{2,}', r'||', text_temp_replaced)\n",
        "    # Split text content to list\n",
        "    text_list = text_temp.split('||')\n",
        "\n",
        "    # Sometimes tika is not able to split rows, then we collapse the delimiter.\n",
        "    if len(text_list) < 3:\n",
        "        text_temp = re.sub(r'\\n{1,}', r'||', text_temp_replaced)\n",
        "        # Split text content to list\n",
        "        text_list = text_temp.split('||')\n",
        "\n",
        "    text_part = ''\n",
        "\n",
        "    '''\n",
        "    First we will capture certification block only.\n",
        "    If that is not found then we will search for certifi keyword and add a fuzzy logic to it.\n",
        "\n",
        "    '''\n",
        "    ''' PART 1:  '''\n",
        "\n",
        "    tag = 'certifications'\n",
        "    certification_headers = ['certifications:', 'certifications', 'certification:', 'certification']\n",
        "    notags = ['education & certification', 'education/certification', 'education and certification', 'education / certification', 'education/ certification', 'education /certification',\n",
        "              'for certification', 'in certification', 'on certification', 'get certifi', 'with certifi', 'become certifi', 'and certifications', '& certifications']\n",
        "\n",
        "    # Capture certification block\n",
        "    for idi, i in enumerate(text_list):\n",
        "        if i.lower().find(tag) > -1:\n",
        "            if any(nt in i.lower() for nt in notags):\n",
        "                #print(\"Skipping because of misleading tags (notags)..\")\n",
        "                continue\n",
        "            else:\n",
        "                # if list element is the header\n",
        "                if len(clean_multi_spaces_alphanum(i)) <= 15:\n",
        "                    text_part = text_list[idi+1]\n",
        "                    break\n",
        "                else:\n",
        "                    text_part = text_list[idi]\n",
        "                    break\n",
        "\n",
        "    # Clean certification block captured above\n",
        "    text_part = re.sub(r'\\uf0b7', r'\\n', text_part)\n",
        "    text_part = re.sub(r'\\n{1,}', r'||', text_part)\n",
        "\n",
        "    out_block = []\n",
        "    unformatted_out = []\n",
        "    if text_part != '':\n",
        "        text_part_list = text_part.split('||')\n",
        "        # Remove certification tag and any previous junks\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            if i.lower().find(tag) > -1:\n",
        "                text_part_list = text_part_list[idi+1:]\n",
        "        # Clean values\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            out_block.append(clean_multi_spaces_alphanum(i))\n",
        "\n",
        "    # Save unformatted output for subtraction in education block.\n",
        "    unformatted_out = out_block\n",
        "\n",
        "\n",
        "\n",
        "    ''' PART 2:  '''\n",
        "\n",
        "    if len(out_block) == 0:\n",
        "        #print(\"Could not find certification block. Using tag search..\")\n",
        "        tag = 'certifi'\n",
        "        text_part_list = []\n",
        "        out_block = []\n",
        "        unformatted_out = []\n",
        "        # Make split more granular since we need sentences and not words\n",
        "        text_temp = re.sub(r'\\n{1,}', r'||', text_temp_replaced)\n",
        "        # Split text content to list\n",
        "        text_list = text_temp.split('||')\n",
        "\n",
        "        # Find all elements with keyword matches.\n",
        "        for idx, i in enumerate(text_list):\n",
        "            if i.lower().find(tag) > -1:\n",
        "                text_part_list.append(i)\n",
        "\n",
        "        # Remove notags from list\n",
        "        popid=[]\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            if any(nt in i.lower() for nt in notags):\n",
        "                popid.append(idi)\n",
        "        text_part_list = [i for j, i in enumerate(text_part_list) if j not in popid]\n",
        "\n",
        "        # Remove original header (if any) from list\n",
        "        popid=[]\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            if any(c ==clean_multi_spaces_alphanum(i).lower() for c in certification_headers):\n",
        "                popid.append(idi)\n",
        "        text_part_list = [i for j, i in enumerate(text_part_list) if j not in popid]\n",
        "\n",
        "        # Remove huge elements if certification is captured in some other contexts\n",
        "        popid=[]\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            if len(i) > 100:\n",
        "                popid.append(idi)\n",
        "        text_part_list = [i for j, i in enumerate(text_part_list) if j not in popid]\n",
        "\n",
        "        # Clean values\n",
        "        for idi, i in enumerate(text_part_list):\n",
        "            temp = re.sub(r'\\uf0b7', r'', i)\n",
        "            out_block.append(clean_multi_spaces_alphanum(temp))\n",
        "\n",
        "        # Save unformatted output for subtraction in education block.\n",
        "        unformatted_out = out_block\n",
        "\n",
        "\n",
        "        ''' PART 2.1:     Fuzzy Logic to combine duplicates'''\n",
        "        if len(out_block) > 1:\n",
        "            # Use fuzzy logic to remove duplicates\n",
        "            df = pd.DataFrame(index = out_block, columns = out_block)\n",
        "            for i in out_block:\n",
        "                for j in out_block:\n",
        "                    df.loc[i, j] = fuzz.partial_ratio(i,j)\n",
        "            df = df.where(np.triu(np.ones(df.shape)).astype(np.bool))\n",
        "            df = df.stack().reset_index()\n",
        "            df.columns = ['Row','Column','Value']\n",
        "            fuzzy_cut_off = 96\n",
        "            sim_words = []\n",
        "            for i in range(len(df)):\n",
        "                if (df.loc[i,'Row'] != df.loc[i,'Column']) and (df.loc[i,'Value'] > fuzzy_cut_off):\n",
        "                    sim_words.append((df.loc[i,'Row'], df.loc[i,'Column']))\n",
        "            remove_sim_words = []\n",
        "            for i in sim_words:\n",
        "                if len(i[0]) >= len(i[1]):\n",
        "                    remove_sim_words.append(i[1])\n",
        "                else:\n",
        "                    remove_sim_words.append(i[0])\n",
        "            # Remove the elements from final list\n",
        "            for i in remove_sim_words:\n",
        "                out_block.remove(i)\n",
        "\n",
        "\n",
        "    # Drop any list element which is blank\n",
        "    if len(out_block) > 1:\n",
        "        popid = []\n",
        "        for idi, i in enumerate(out_block):\n",
        "            if len(i) <= 1:\n",
        "                popid.append(idi)\n",
        "        out_block = [i for j, i in enumerate(out_block) if j not in popid]\n",
        "\n",
        "    # return formatted output\n",
        "    bullet = '• '\n",
        "    if len(out_block) > 1:\n",
        "        out_block = [bullet+item for item in out_block]\n",
        "\n",
        "    if len(out_block) > 0:\n",
        "        out = '\\n'.join(out_block)\n",
        "#        print(out)\n",
        "#        print(unformatted_out)\n",
        "        return [{\"inference\":resume_output_cleaner(out, \"certificate\"), \"unformatted_inf\":resume_output_cleaner(unformatted_out, \"certificate\")}]\n",
        "    else:\n",
        "        #print(\"Not Found\")\n",
        "        return [{\"inference\": ''}]\n",
        "\n",
        "def feeder_linked_in(file_path, text_content):\n",
        "\n",
        "    # original_text = text_content.split('\\n')\n",
        "    original_text = get_first_block_from_resume(text_content)\n",
        "    new_text_list = []\n",
        "    email_matches = []\n",
        "    linkedin_matches = []\n",
        "    for each_line in original_text:\n",
        "         if each_line != '' :\n",
        "            new_text_list.append(each_line.replace ('\\t',' ').encode('ascii','ignore').decode('utf-8'))\n",
        "    for sentence in new_text_list:\n",
        "        if email_pattern.search(sentence):\n",
        "            match = email_pattern.search(sentence)\n",
        "            if match.group(0) not in email_matches:\n",
        "                email_matches.append(match.group(0))\n",
        "        if linkedin_pattern.search(sentence):\n",
        "            match = linkedin_pattern.search(sentence)\n",
        "\n",
        "            if match.group(0) not in linkedin_matches :\n",
        "                linkedin_matches.append(match.group(0))\n",
        "\n",
        "    final_output = []\n",
        "    for each_answer in linkedin_matches:\n",
        "        if \"/in/\" in each_answer:\n",
        "            each_answer = resume_output_cleaner(each_answer,\"linkedin\")\n",
        "            final_output.append({'inference':each_answer.strip()})\n",
        "\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def feeder_email(file_path, text_content):\n",
        "\n",
        "    original_text = text_content.lower()\n",
        "    original_text = get_first_block_from_resume(original_text)\n",
        "    new_text_list = []\n",
        "    email_matches = []\n",
        "    # linkedin_matches = []\n",
        "    for each_line in original_text:\n",
        "        if each_line != '' :\n",
        "            new_text_list.append(each_line.replace ('\\t',' ').encode('ascii','ignore').decode('utf-8'))\n",
        "    for sentence in new_text_list:\n",
        "        if email_pattern.search(sentence):\n",
        "            match = email_pattern.search(sentence)\n",
        "            if match.group(0) not in email_matches:\n",
        "                email_matches.append(match.group(0))\n",
        "\n",
        "    email_matches = list(set(email_matches))\n",
        "    final_output = []\n",
        "    for each_answer in list(set(email_matches)):\n",
        "        each_answer = resume_output_cleaner(each_answer,\"candidate_email\")\n",
        "        # final_output.append({'inference':each_answer.strip()})\n",
        "        final_output.append(each_answer.strip())\n",
        "    # print(final_output)\n",
        "    output_list = [{'inference':str(\",\".join(s.strip() for s in final_output))}]\n",
        "    # print(output_list)\n",
        "    # return final_output\n",
        "    return output_list\n",
        "\n",
        "\n",
        "def feeder_phone(file_path,text_content):\n",
        "    text_content = \"\\n\".join(get_first_block_from_resume(text_content))\n",
        "\n",
        "    text_content = '\\n'.join(get_cleaned_encoded_text_from_jd(text_content))\n",
        "\n",
        "    pattern = re.compile(r'[(+]?\\d+[)]?[ \\t\\f\\v\\r]*[.-]?[ \\t\\v\\f\\r]*[(]?\\d{2,}[)]?[ \\t\\v\\f\\r]*[.-]?[ \\t\\f\\v\\r]*[(]?\\d{2,}[)]?[ \\t\\f\\r\\v]*[.-]?[ \\t\\v\\f\\r]*[(]?\\d+[)]?[ \\t\\v\\r\\f]*')\n",
        "    extractor = URLExtract()\n",
        "    email_pattern = re.compile(r'([\\w\\-\\.]+@(\\w[\\w\\-]+\\.)+[\\w\\-]+)')\n",
        "\n",
        "    extractor = URLExtract()\n",
        "    urls = extractor.find_urls(text_content)\n",
        "\n",
        "    if len(urls) > 0:\n",
        "        for url in urls:\n",
        "            text_content = text_content.replace(url,' ')\n",
        "\n",
        "    text_content = email_pattern.sub(' ',text_content)\n",
        "    # print(text_content)\n",
        "\n",
        "    answer = []\n",
        "    result = re.findall(pattern,text_content)\n",
        "    for each_match in result:\n",
        "        phone_no_only_digits = []\n",
        "        each_match_split = list(each_match)\n",
        "        for each in each_match_split:\n",
        "            string = ''\n",
        "            if each in ['1','2','3','4','5','6','7','8','9','0']:\n",
        "                phone_no_only_digits.append(each)\n",
        "            else:\n",
        "                continue\n",
        "        if len(phone_no_only_digits) >= 10:\n",
        "            for each in phone_no_only_digits:\n",
        "                string = string + each\n",
        "\n",
        "            each_match_sp_char_removed = re.sub(r'[-()+_]',' ',each_match)\n",
        "            split_with_space = each_match_sp_char_removed.split()\n",
        "            # print(split_with_space)\n",
        "            phone_num_cleaned = each_match\n",
        "            if len(split_with_space) > 3:\n",
        "                phone_num_cleaned = ''\n",
        "                for each_chunk_of_numbers in split_with_space:\n",
        "                    no_of_num_in_chunk = list(each_chunk_of_numbers)\n",
        "                    # print(no_of_num_in_chunk)\n",
        "                    if len(no_of_num_in_chunk) <= 4:\n",
        "                        phone_num_cleaned = phone_num_cleaned + \" \" + each_chunk_of_numbers\n",
        "\n",
        "            answer.append(phone_num_cleaned)\n",
        "    # print(answer)\n",
        "    output = list(set(answer))\n",
        "    final_output = []\n",
        "    for each_answer in output:\n",
        "        each_answer = resume_output_cleaner(each_answer,\"candidate_phone_number\")\n",
        "        # final_output.append({'inference':each_answer})\n",
        "        final_output.append(each_answer)\n",
        "    # print(final_output)\n",
        "    output_list = [{'inference':str(\",\".join(s.strip() for s in final_output))}]\n",
        " #  print(output_list)\n",
        "    # return final_output\n",
        "    return output_list\n",
        "\n",
        "#searching whether file exists and return path of file\n",
        "#def find(name, path):\n",
        " #   for root, dirs, files in os.walk(path):\n",
        "  #      if name in files:\n",
        "   #         return os.path.join(root, name)\n",
        "\n",
        "df_csv = pd.read_csv('/home/drive/Downloads/ground_truth_2.csv', index_col=False)\n",
        "path='/home/drive/Downloads/gr_resumes'\n",
        "#name = input(\"Enter your resume name : \")\n",
        "#parse=find(name,path)\n",
        "j=0\n",
        "\n",
        "#RESULTS after testing\n",
        "def result_list_name():\n",
        "    result_list_name=[]\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if os.path.isfile(os.path.join(path,file)) and '.~lock.' in file:\n",
        "                continue\n",
        "            parse=os.path.join(root, file)\n",
        "            parsed = unpack.from_file(parse)\n",
        "            text_content = parsed['content']\n",
        "            result_list_name.append(file)\n",
        "        return result_list_name\n",
        "n=result_list_name()\n",
        "\n",
        "#RESULTS after testing\n",
        "def result_list_phone():\n",
        "    result_list_phone=[]\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if os.path.isfile(os.path.join(path,file)) and '.~lock.' in file:\n",
        "                continue\n",
        "            parse=os.path.join(root, file)\n",
        "            parsed = unpack.from_file(parse)\n",
        "            text_content = parsed['content']\n",
        "            phone_no_extracted=feeder_phone('',text_content)\n",
        "            ph_num_extract=phone_no_extracted[0]['inference']\n",
        "\n",
        "            result_phone_no=compare_phone(file,ph_num_extract)\n",
        "            result_list_phone.append(result_phone_no)\n",
        "\n",
        "     #   print(len(result_list_phone))\n",
        "        return result_list_phone\n",
        "p=result_list_phone()\n",
        "#print(p)\n",
        "phone_correct=0\n",
        "\n",
        "# To count the no of correct matches\n",
        "for i in p:\n",
        "    if i==1:\n",
        "        phone_correct+=1\n",
        "print(phone_correct)\n",
        "\n",
        "#RESULTS after testing\n",
        "def result_list_email():\n",
        "    result_list_email=[]\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if os.path.isfile(os.path.join(path,file)) and '.~lock.' in file:\n",
        "                continue\n",
        "            parse=os.path.join(root, file)\n",
        "            parsed = unpack.from_file(parse)\n",
        "            text_content = parsed['content']\n",
        "            email_extracted=feeder_email('',text_content)\n",
        "            email_extract=email_extracted[0]['inference']\n",
        "\n",
        "            result_email=compare_email(file,email_extract)\n",
        "           # re=int(result_email)\n",
        "            result_list_email.append(result_email)\n",
        "\n",
        "      #  print(len(result_list_email))\n",
        "        return result_list_email\n",
        "e=result_list_email()\n",
        "#print(e)\n",
        "\n",
        "# To count the no of correct matches\n",
        "email_correct=0\n",
        "for i in e:\n",
        "    if i==1:\n",
        "        email_correct+=1\n",
        "print(email_correct)\n",
        "\n",
        "\n",
        "#RESULTS after testing\n",
        "def result_list_linked_in():\n",
        "    result_list_linked_in=[]\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if os.path.isfile(os.path.join(path,file)) and '.~lock.' in file:\n",
        "                continue\n",
        "            parse=os.path.join(root, file)\n",
        "            parsed = unpack.from_file(parse)\n",
        "            text_content = parsed['content']\n",
        "            linked_in=feeder_linked_in('',text_content)\n",
        "\n",
        "            result_linked_in=compare_linked_in(file,linked_in)\n",
        "\n",
        "            result_list_linked_in.append(result_linked_in)\n",
        "\n",
        "      #  print(len(result_list_linked_in))\n",
        "        return result_list_linked_in\n",
        "l=result_list_linked_in()\n",
        "#print(l)\n",
        "\n",
        "# To count the no of correct matches\n",
        "linked_in_correct=0\n",
        "for i in l:\n",
        "    if i==1:\n",
        "        linked_in_correct+=1\n",
        "print(linked_in_correct)\n",
        "\n",
        "\n",
        "# results after certificate testing\n",
        "def result_list_certificate():\n",
        "    result_list_certificate=[]\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if os.path.isfile(os.path.join(path,file)) and '.~lock.' in file:\n",
        "                continue\n",
        "            parse=os.path.join(root, file)\n",
        "            parsed = unpack.from_file(parse)\n",
        "            text_content = parsed['content']\n",
        "            certificate_extracted=feeder_certificate('',text_content)\n",
        "            certficate=certificate_extracted[0]['inference']\n",
        "\n",
        "            result_certificate=compare_certificate(file,certficate)\n",
        "\n",
        "            result_list_certificate.append(result_certificate)\n",
        "\n",
        "      #  print(len(result_list_email))\n",
        "        return result_list_certificate\n",
        "c=result_list_certificate()\n",
        "#print(c)\n",
        "certificate_correct=0\n",
        "\n",
        "# To count the no of correct matches\n",
        "for i in c:\n",
        "    if i==1:\n",
        "        certificate_correct+=1\n",
        "print(certificate_correct)\n",
        "\n",
        "\n",
        "# USING FLAGS to display the data frame\n",
        "\n",
        "flags = {\n",
        "    'ph_no_flag':1,\n",
        "    'email_flag':1,\n",
        "    'linked_in_flag':0,\n",
        "    'certificate_flag':0\n",
        "     }\n",
        "\n",
        "#single condition\n",
        "if flags['ph_no_flag']==1 and flags['email_flag']==0 and flags['linked_in_flag']==0 and flags['certificate_flag']==0:\n",
        "    df_phone = {'phone':p, 'resume':n }\n",
        "    data_phone=pd.DataFrame(df_phone)\n",
        "    print(data_phone.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['ph_no_flag']==1 and flags['email_flag']==1 and flags['linked_in_flag']==0 and flags['certificate_flag']==0:\n",
        "    df_phone_email = {'phone':p, 'resume':n , 'email':e }\n",
        "    data_phone_email=pd.DataFrame(df_phone_email)\n",
        "    print(data_phone_email.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['ph_no_flag']==1 and flags['email_flag']==0 and flags['linked_in_flag']==1 and flags['certificate_flag']==0:\n",
        "    df_phone_email = {'phone':p, 'resume':n , 'linked_in':l }\n",
        "    data_phone_email=pd.DataFrame(df_phone_email)\n",
        "    print(data_phone_email.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['ph_no_flag']==1 and flags['email_flag']==0 and flags['linked_in_flag']==0 and flags['certificate_flag']==1:\n",
        "    df_phone_email = {'phone':p, 'resume':n ,  'certificates': c }\n",
        "    data_phone_email=pd.DataFrame(df_phone_email)\n",
        "    print(data_phone_email.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['email_flag']==1 and flags['ph_no_flag']==0 and flags['linked_in_flag']==0 and flags['certificate_flag']==0:\n",
        "    df_email = {'email':e , 'resume':n }\n",
        "    data_email = pd.DataFrame(df_email)\n",
        "    print(data_email.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "if flags['email_flag']==1 and flags['ph_no_flag']==0 and flags['linked_in_flag']==1 and flags['certificate_flag']==0:\n",
        "    df_email_linked_in = {'email':e , 'resume':n , 'linked_in':l }\n",
        "    data_email_linked_in = pd.DataFrame(df_email_linked_in)\n",
        "    print(data_email_linked_in.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['email_flag']==1 and flags['ph_no_flag']==0 and flags['linked_in_flag']==0 and flags['certificate_flag']==1:\n",
        "    df_email_certificate = {'email':e , 'resume':n , 'certificates':c }\n",
        "    data_email_certificate = pd.DataFrame(df_email_certificate)\n",
        "    print(data_email_certificate.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "if flags['linked_in_flag']==1 and flags['ph_no_flag']==0 and flags['email_flag']==0 and flags['certificate_flag']==0 :\n",
        "    df_linked_in= {'linked_in':l , 'resume':n }\n",
        "    data_linked_in =pd.DataFrame(df_linked_in)\n",
        "    print(data_linked_in.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "if flags['linked_in_flag']==1 and flags['ph_no_flag']==0 and flags['email_flag']==0 and flags['certificate_flag']==1 :\n",
        "    df_linked_in= {'linked_in':l , 'resume':n , 'certificates':c }\n",
        "    data_linked_in =pd.DataFrame(df_linked_in)\n",
        "    print(data_linked_in.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "if flags['certificate_flag']==1 and flags['ph_no_flag']==0 and flags['linked_in_flag']==0 and flags['email_flag']==0:\n",
        "    df_certificate = {'certificate':c , 'resume':n }\n",
        "    data_certificate =pd.DataFrame(df_certificate)\n",
        "    print(data_certificate.to_string(index=False))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X_EncCteCUj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (other-env)",
      "language": "python",
      "name": "env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}